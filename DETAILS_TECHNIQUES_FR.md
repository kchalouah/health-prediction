# D√©tails Techniques & Explication du Projet

Ce document fournit une explication d√©taill√©e de chaque composant, d√©pendance et fichier du projet **Endpoint Sentinel AI**.

---

## üì¶ 1. Les D√©pendances (`requirements.txt`)

Ce fichier liste toutes les biblioth√®ques Python tiers n√©cessaires au fonctionnement du projet. Voici pourquoi nous utilisons chacune d'elles :

### Serveur & API
*   **`fastapi`** : Un framework web moderne et tr√®s rapide.
    *   *Pourquoi ?* : Pour construire l'API Backend qui re√ßoit les m√©triques des agents et sert les donn√©es au tableau de bord. Il g√®re la validation des donn√©es automatiquement (via Pydantic).
*   **`uvicorn`** : Un serveur web ASGI.
    *   *Pourquoi ?* : FastAPI est l'application, mais Uvicorn est le "moteur" qui la fait tourner en production. Il est optimis√© pour les applications asynchrones (`async/await`).
*   **`requests`** : Une biblioth√®que pour envoyer des requ√™tes HTTP.
    *   *Pourquoi ?* : Utilis√©e par le **simulateur** de trafic (`traffic_gen.py`) pour envoyer des fausses donn√©es (`POST`) √† l'API backend comme si c'√©tait un vrai agent.

### Traitement de Donn√©es
*   **`pandas`** : La r√©f√©rence pour la manipulation de donn√©es tabulaires.
    *   *Pourquoi ?* : Indispensable pour l'IA. Nous recevons des m√©triques brutes, nous les mettons dans un "DataFrame" Pandas pour calculer des moyennes glissantes, des √©carts-types, et pr√©parer les donn√©es pour le mod√®le.
*   **`numpy`** : Calcul num√©rique haute performance.
    *   *Pourquoi ?* : Utilis√© en interne par Pandas et Scikit-Learn pour les op√©rations matricielles rapides.

### Intelligence Artificielle (ML/DL)
*   **`scikit-learn`** : Biblioth√®que classique de Machine Learning.
    *   *Pourquoi ?* : Fournit l'algorithme **Isolation Forest** utilis√© pour la d√©tection d'anomalies (d√©tecter ce qui est "bizarre" sans l'avoir jamais vu).
*   **`xgboost`** : Biblioth√®que de "Gradient Boosting".
    *   *Pourquoi ?* : Fournit le mod√®le **RiskClassifier**. C'est souvent plus performant et rapide que les Random Forests classiques pour les donn√©es structur√©es (tableaux).
*   **`torch` (PyTorch)** : Framework de Deep Learning.
    *   *Pourquoi ?* : Utilis√© pour le mod√®le **LSTM** (Long Short-Term Memory). C'est un r√©seau de neurones capable de comprendre les s√©quences temporelles (le pass√© influence le futur), id√©al pour la pr√©diction de tendance.
*   **`joblib`** : Outil de s√©rialisation.
    *   *Pourquoi ?* : Permet de "sauvegarder" un mod√®le entra√Æn√© dans un fichier (`.joblib` ou `.json`) et de le "charger" instantan√©ment au d√©marrage, √©vitant de devoir r√©-entra√Æner l'IA √† chaque lancement.

### Interface & Visualisation
*   **`streamlit`** : Framework pour cr√©er des applications de donn√©es en pur Python.
    *   *Pourquoi ?* : Permet de cr√©er le **Dashboard** interactif (`dashboard/app.py`) sans √©crire une seule ligne de HTML, CSS ou JavaScript.
*   **`plotly`** : Biblioth√®que de graphiques interactifs.
    *   *Pourquoi ?* : Utilis√© dans Streamlit pour afficher la "Matrice de Risque" dynamique et les Heatmaps, permettant de zoomer et survoler les points.

### Monitoring & Syst√®me
*   **`prometheus_client`** : Client pour Prometheus.
    *   *Pourquoi ?* : Permet √† notre code Python d'exposer ses propres m√©triques (ex: `endpoint_risk_prob`) sur une page `/metrics` pour qu'une base de donn√©es Prometheus puisse les aspirer.
*   **`psutil`** : Process & System Utilities.
    *   *Pourquoi ?* : C'est les "yeux" de l'agent. Il permet au code Python de lire l'utilisation r√©elle du CPU, de la RAM, et du Disque du syst√®me h√¥te.
*   **`gputil`** : Utilitaire GPU.
    *   *Pourquoi ?* : Permet de d√©tecter la pr√©sence d'une carte graphique NVIDIA et de lire son taux d'utilisation.

### Infrastructure & Utilitaires
*   **`sqlalchemy`** : ORM (Object Relational Mapper) pour bases de donn√©es.
    *   *Pourquoi ?* : Permet d'interagir avec la base de donn√©es SQL (`endpoint.db`) en utilisant des classes Python (`Model`) au lieu d'√©crire du SQL brut. C'est plus propre et s√©curis√©.
*   **`apscheduler`** : Planificateur de t√¢ches avanc√©.
    *   *Pourquoi ?* : Permet de lancer la boucle de surveillance (`monitor_loop`) en arri√®re-plan toutes les 5 secondes sans bloquer l'API principale qui doit rester disponible pour r√©pondre aux requ√™tes.

---

## üìÇ 2. Structure des Fichiers et R√¥les

Ce projet est d√©coup√© en modules logiques (Architecture Micro-services).

### üß† Backend (`/backend`)
C'est le cerveau du syst√®me.
*   **`main.py`** : Le chef d'orchestre.
    *   D√©marre le serveur API.
    *   Charge les mod√®les ML au d√©marrage.
    *   Lance le planificateur (Scheduler) pour analyser les donn√©es p√©riodiquement.
    *   D√©finit les "Routes" (URL) comme `/api/metrics` (recevoir les donn√©es) ou `/api/dashboard` (envoyer l'√©tat au frontend).
*   **`collector.py`** : Les sens.
    *   Contient la classe `SystemMonitor`. Son seul but est d'utiliser `psutil` pour interroger le syst√®me d'exploitation et retourner un dictionnaire propre (CPU, RAM, etc.).
*   **`security_mon.py`** : Le chien de garde.
    *   Surveille le fichier de logs g√©n√©r√© par l'agent **Osquery** (qui tourne s√©par√©ment). Il cherche des mots-cl√©s sp√©cifiques dans les logs JSON pour d√©tecter des √©v√©nements de s√©curit√©.
*   **`database.py`** : La m√©moire.
    *   Configure la connexion √† la base de donn√©es SQLite.
    *   D√©finit la structure des tables (`EndpointMetric`, `SecurityEvent`) pour que SQLAlchemy sache comment stocker les donn√©es.

### ü§ñ Intelligence Artificielle (`/ml`)
*   **`models.py`** : Les comp√©tences.
    *   Contient les classes Python qui enveloppent les algorithmes complexes (`RiskClassifier`, `AnomalyDetector`, `HealthForecaster`). Cela permet d'utiliser `model.predict()` simplement dans le backend sans se soucier des maths complexes derri√®re.
*   **`feature_engine.py`** : Le traducteur.
    *   Les mod√®les ML ne comprennent pas bien les chiffres bruts isol√©s. Ce fichier transforme "CPU: 80%" en "Moyenne CPU 1h: 40%, Tendance: +10%". C'est ce qu'on appelle l'ing√©nierie des fonctionnalit√©s (Feature Engineering).
*   **`health_scorer.py`** : Le juge.
    *   Prend les r√©sultats de tous les mod√®les (Probabilit√© de risque, Bool√©en d'anomalie) et applique une logique m√©tier pour donner une note finale sur 100 et g√©n√©rer une phrase de recommandation humaine (ex: "Tuer le processus").
*   **`train.py`** : Le professeur.
    *   Script ex√©cut√© au d√©marrage pour g√©n√©rer des donn√©es synth√©tiques (fausses mais r√©alistes) et entra√Æner les mod√®les. Cela assure que le syst√®me fonctionne "out-of-the-box" sans attendre des semaines de collecte de donn√©es.

### üìä Dashboard (`/dashboard`)
*   **`app.py`** : Le visage.
    *   Un script Streamlit autonome. Il boucle infiniment : il demande l'√©tat actuel au Backend via API, puis redessine toute la page avec les graphiques √† jour.

### ‚öôÔ∏è Configuration & Infrastructure
*   **`docker-compose.yml`** : Le plan de construction.
    *   D√©finit comment lancer tous ces scripts (Backend, Dashboard, Base de donn√©es Prometheus) dans des conteneurs isol√©s mais connect√©s entre eux.
*   **`osquery.conf`** : Le r√®glement.
    *   Fichier de configuration pour l'agent Osquery. Il contient les requ√™tes SQL qui d√©finissent ce qu'il faut surveiller (ex: "Surveille les processus qui √©coutent sur le r√©seau").
*   **`prometheus.yml`** : La configuration d'archivage.
    *   Dit √† la base de donn√©es Prometheus : "Va chercher les m√©triques sur `backend:8000/metrics` toutes les 5 secondes".
